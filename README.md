# awesome-trustworthy-LLMs
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/hee9joon/Awesome-Diffusion-Models) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/chetanraj/awesome-github-badges)

This repository contains a collection of resources and papers on ***trustworthy Large Language Models***.

## Contents
- [Resources](#resources)
- [Papers](#papers)
  - [Survey](#survey)
  - [Explainability](#explainability)  
  - [Security](#security)
  - [Robustness](#robustness)
  - [Privacy](#privacy)
  - [Fairness](#fairness)
- 
# Resources
## Introductory Posts
**GPT-4 is OpenAI’s most advanced system, producing safer and more useful responses.** \
*OpenAI* \
[[Website](https://openai.com/product/gpt-4)] \
14 Mar 2023

**Princeton COS 597G (Fall 2022): Understanding Large Language Models** \
*Danqi Chen* \
[[Website](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/lectures/)]

**Stanford CS324 - Large Language Models** \
*Percy Liang,
Tatsunori Hashimoto, Christopher Ré,
 Rishi Bommasani, Sang Michael Xie* \
[[website](https://stanford-cs324.github.io/winter2022/)]
# Papers

## Survey

**A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity** \
*Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, Pascale Fung* \
28 Feb 2023. [[Paper](https://arxiv.org/abs/2302.04023)] 

## Explainability

## Security

**Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks** \
*Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, Tatsunori Hashimoto* \
11 Feb 2023. [[Paper](https://arxiv.org/abs/2302.05733)] 

**Probing Toxic Content in Large Pre-Trained Language Models**
*Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, Dit-Yan Yeung*
ACL 2021 [[Paper](https://aclanthology.org/2021.acl-long.329/)]

**More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models** \
*Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, Mario Fritz* \
23 Feb 2023. [[Paper](https://arxiv.org/abs/2302.12173)] 

**Ignore Previous Prompt: Attack Techniques For Language Models** \
*Fábio Perez, Ian Ribeiro*
17 Nov 2022 [[Paper](https://arxiv.org/abs/2211.09527)]
## Robustness

*Adversarial Prompting for Black Box Foundation Models***
*Natalie Maus, Patrick Chao, Eric Wong, Jacob Gardner*
8 Feb 2023 [[Paper](https://arxiv.org/abs/2302.04237)]

## Privacy

**On the Feasibility of Specialized Ability Stealing for Large Language Code Models** \
*Zongjie Li, Chaozheng Wang, Pingchuan Ma, Chaowei Liu, Shuai Wang, Daoyuan Wu, Cuiyun Gao* \
6 Mar 2023. [[Paper](https://arxiv.org/abs/2303.03012)] 


**Large Language Models Can Be Strong Differentially Private Learners** \
*Xuechen Li, Florian Tramèr, Percy Liang, Tatsunori Hashimoto* \
ICLR 2022 [[Paper](https://arxiv.org/abs/2110.05679)]

**Differentially Private Natural Language Models: Recent Advances and Future Directions** \
*Lijie Hu, Ivan Habernal, Lei Shen, Di Wang* \
[[Paper](https://arxiv.org/abs/2301.09112)]

**Differentially Private Language Models for Secure Data Sharing** \
*Justus Mattern, Zhijing Jin, Benjamin Weggenmann, Bernhard Schoelkopf, Mrinmaya Sachan* \
EMNLP 2022 [[Paper](https://arxiv.org/abs/2210.13918)] 

**Differentially Private Fine-tuning of Language Models** \
*Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, Huishuai Zhang* \
**Extracting Training Data from Large Language Models** \
ICLR 2022 [[Paper](https://arxiv.org/abs/2110.06500)]

**Extracting Training Data from Large Language Models**
*Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, Colin Raffel* \
USENIX 2021 [[Paper](https://arxiv.org/abs/2012.07805)]

**The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks** \
*Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, Dawn Song* \
USENIX 2019 [[Paper](https://arxiv.org/abs/1802.08232)]

**Can Foundation Models Help Us Achieve Perfect Secrecy?** \
*Simran Arora, Christopher Ré* \
Jan 2023 [[Paper](Can Foundation Models Help Us Achieve Perfect Secrecy?)]

**Reconstruction Attack on Instance Encoding for Language Understanding** \
*Shangyu Xie, Yuan Hong* \
EMNLP 2021 [[Paper](https://aclanthology.org/2021.emnlp-main.154/)]

**Comprehensive Privacy Analysis of Deep Learning**\
*Milad Nasr, Reza Shokri, Amir Houmansadr*\
Dec 2018 [[Paper](https://arxiv.org/abs/1812.00910)]

## Fairness

